{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.67.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jacob/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/jacob/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/jacob/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/jacob/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'List all records in the products table where price is greater than $20'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY_PATTERNS = {\n",
    "    \"EXAMPLE\": [\"example\", \"examples\"],\n",
    "    \"UPLOAD\": [\"upload\", \"add\"],\n",
    "    \"SELECT\": [\"select\", \"columns\", \"fields\", \"retrieve\", \"get\", \"show\"],\n",
    "    \"AGGREGATE\": ['many', 'sum', 'average', 'mean', 'count'],\n",
    "    \"GROUP BY\": [\"group by\", \"aggregate\", \"group\", 'grouping', 'each', 'total'],\n",
    "    \"HAVING\": [\"having\", \"condition\", \"filter\", 'over', 'under', 'less', 'greater'],\n",
    "    \"ORDER BY\": [\"order by\", \"sort\", \"ascending\", \"descending\"],\n",
    "    \"WHERE\": [\"where\", \"condition\", \"filter\", 'over', 'under', 'less', 'greater', 'who'],\n",
    "    \"JOIN\": [\"join\", \"merge\", \"combine\", \"foreign key\", \"on\"]\n",
    "}\n",
    "\n",
    "QUERY_PATTERNS = {\n",
    "    \"EXAMPLE\": [\"example\", \"examples\", \"sample\", \"instance\", \"demo\", \"case\"],\n",
    "    \"UPLOAD\": [\"upload\", \"add\", \"insert\", \"import\", \"save\", \"load\", \"store\"],\n",
    "    \"SELECT\": [\"select\", \"columns\", \"fields\", \"retrieve\", \"get\", \"show\", \"fetch\", \"extract\",\n",
    "               \"display\", \"list\", \"view\", \"pick\", \"choose\", \"read\", \"query\"],\n",
    "    \"AGGREGATE\": [\"many\", \"sum\", \"average\", \"mean\", \"count\", \"total\", \"maximum\", \"minimum\",\n",
    "                  \"min\", \"max\", \"avg\", \"median\", \"aggregate\", \"statistics\", \"metrics\"],\n",
    "    \"GROUP BY\": [\"group by\", \"aggregate\", \"group\", \"grouping\", \"each\", \"total\", \"categorize\",\n",
    "                 \"partition\", \"classify\", \"segment\", \"cluster\", \"bucket\"],\n",
    "    \"HAVING\": [\"having\", \"condition\", \"filter\", \"over\", \"under\", \"less\", \"greater\", \"limit\", 'more',\n",
    "               \"range\", \"restrict\", \"criteria\", \"threshold\", \"constraint\", \"above\", \"below\"],\n",
    "    \"ORDER BY\": [\"order by\", \"sort\", \"ascending\", \"descending\", \"rank\", \"arrange\", \"prioritize\",\n",
    "                 \"sequence\", \"order\", \"hierarchy\", \"top\", \"bottom\", \"sorted\"],\n",
    "    \"WHERE\": [\"where\", \"condition\", \"filter\", \"over\", \"under\", \"less\", \"greater\", \"who\",\n",
    "              \"which\", \"that\", \"criteria\", \"subset\", \"limit\", \"restrict\", \"search\",\n",
    "              \"match\", \"constraint\", 'in the'],\n",
    "    \"JOIN\": [\"join\", \"merge\", \"combine\", \"foreign key\", \"unite\", \"relate\", \"link\",\n",
    "             \"connect\", \"associate\", \"bridge\", \"union\", \"inner\", \"outer\", \"left\", \"right\"],\n",
    "    \"LIMIT\": ['top', 'bottom', 'highest', 'lowest', 'biggest', 'smallest', 'limit'],\n",
    "    \"SQL\": ['sql', 'mysql'],\n",
    "    \"MONGODB\": ['mongo', 'mongodb', 'nosql']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "example_inputs = [\n",
    "    \"Show me all the records in the employees table\",  # Select\n",
    "    \"Upload this data '../data/sql_data/students.csv'\",  # Upload\n",
    "    \"Show me examples of sql queries\",  # Example\n",
    "    \"Show me examples of sql queries using group by\",  # Example group by\n",
    "    \"Get the details of employees who work in the Sales department\",  # Where\n",
    "    \"How many employees are there in the company\",  # Select count(*)\n",
    "    \"Show me the names and salaries of employees earning more than $50000\",  # Where\n",
    "    \"List all employees, sorted by their hire date in descending order\",  # Order by\n",
    "    \"Find the average salary of employees in the Engineering department\",  # Select Avg() where\n",
    "    \"How many employees are there in each department\",  # Group by\n",
    "    \"Show me the departments where the total salaries of employees exceed $100000\",  # Having\n",
    "    \"List the products where the average price is less than $50\",  # Having\n",
    "    \"What are the top 10 highest-paid employees\",  # Order by limit\n",
    "    \"Which customers have placed more than 5 orders\",  # Having\n",
    "    \"Find the total revenue generated by each product category\",  # Group by\n",
    "    \"List all orders placed in the last 30 days, sorted by order date\",  # Where Order by\n",
    "    \"Join the employees table with the departments table to find department names\",  # Join\n",
    "    \"List all customers and their orders\",  # Join\n",
    "    \"How many products are in stock for each supplier\",  # Group by\n",
    "    \"Show me the products where the stock quantity is less than 10\",  # Where\n",
    "    \"Upload the file '../data/new_products.json' into the database\",  # Upload\n",
    "    \"Find the maximum salary in each department\",  # Group by Max()\n",
    "    \"List employees grouped by their job titles\",  # Group by\n",
    "    \"Show me all orders where the total exceeds $1000\",  # Where\n",
    "    \"List all customers sorted by their last purchase date\",  # Order by\n",
    "    \"Show me examples of sql queries for finding duplicates\",  # Example\n",
    "    \"How many employees were hired in the last year\",  # Where Count()\n",
    "    \"Find the minimum price of products in each category\",  # Group by Min()\n",
    "    \"Join the orders table with the customers table to get customer details\",  # Join\n",
    "    \"List all records in the products table where price is greater than $20\",  # Where\n",
    "    \"Show me the names of employees earning the top 5 highest salaries\",  # Order by limit\n",
    "    \"Find the sum of sales for each region\",  # Group by Sum()\n",
    "    \"List all products sorted by their price in ascending order\",  # Order by\n",
    "]\n",
    "\n",
    "example_queries = {\n",
    "    (0, \"SELECT\"): \"Show me all the records in the employees table\",  # Select\n",
    "    (1, \"UPLOAD\"): \"Upload this data '../data/sql_data/students.csv'\",  # Upload\n",
    "    (2, \"EXAMPLE\", \"SELECT\", \"SQL\"): \"Show me examples of sql queries\",  # Example\n",
    "    (3, \"EXAMPLE\", \"SELECT\", \"GROUP BY\", \"SQL\"): \"Show me examples of sql queries using group by\",  # Example group by\n",
    "    (4, \"SELECT\", \"WHERE\"): \"Get the details of employees who work in the Sales department\",  # Where\n",
    "    (5, \"SELECT\", \"COUNT\"): \"How many employees are there in the company\",  # Select count(*)\n",
    "    (6, \"SELECT\", \"WHERE\"): \"Show me the names and salaries of employees earning more than $50000\",  # Where\n",
    "    (7, \"SELECT\", \"ORDER BY\"): \"List all employees, sorted by their hire date in descending order\",  # Order by\n",
    "    (8, \"AGGREGATE\", \"WHERE\"): \"Find the average salary of employees in the Engineering department\",  # Select Avg() where\n",
    "    (9, \"SELECT\", \"GROUP BY\"): \"How many employees are there in each department\",  # Group by\n",
    "    (10, \"SELECT\", \"HAVING\"): \"Show me the departments where the total salaries of employees exceed $100000\",  # Having\n",
    "    (11, \"SELECT\", \"HAVING\"): \"List the products where the average price is less than $50\",  # Having\n",
    "    (12, \"SELECT\", \"ORDER BY\", \"LIMIT\"): \"What are the top 10 highest paid employees\",  # Order by limit\n",
    "    (13, \"SELECT\", \"HAVING\"): \"Which customers have placed more than 5 orders\",  # Having\n",
    "    (14, \"SELECT\", \"GROUP BY\"): \"Find the total revenue generated by each product category\",  # Group by\n",
    "    (15, \"SELECT\", \"WHERE\", \"ORDER BY\"): \"List all orders placed in the last 30 days, sorted by order date\",  # Where Order by\n",
    "    (16, \"SELECT\", \"JOIN\"): \"Join the employees table with the departments table to find department names\",  # Join\n",
    "    (17, \"SELECT\", \"JOIN\"): \"List all customers and their orders\",  # Join\n",
    "    (18, \"SELECT\", \"GROUP BY\"): \"How many products are in stock for each supplier\",  # Group by\n",
    "    (19, \"SELECT\", \"WHERE\"): \"Show me the products where the stock quantity is less than 10\",  # Where\n",
    "    (20, \"UPLOAD\"): \"Upload the file '../data/new_products.json' into the database\",  # Upload\n",
    "    (21, \"GROUP BY\", \"AGGREGATE\"): \"Find the maximum salary in each department\",  # Group by Max()\n",
    "    (22, \"SELECT\", \"GROUP BY\"): \"List employees grouped by their job titles\",  # Group by\n",
    "    (23, \"SELECT\", \"WHERE\"): \"Show me all orders where the total exceeds $1000\",  # Where\n",
    "    (24, \"SELECT\", \"ORDER BY\"): \"List all customers sorted by their last purchase date\",  # Order by\n",
    "    (25, \"EXAMPLE\", \"SELECT\", \"SQL\"): \"Show me examples of sql queries for finding duplicates\",  # Example\n",
    "    (26, \"AGGREGATE\", \"WHERE\"): \"How many employees were hired in the last year\",  # Where Count()\n",
    "    (27, \"GROUP BY\", \"AGGREGATE\"): \"Find the minimum price of products in each category\",  # Group by Min()\n",
    "    (28, \"SELECT\", \"JOIN\"): \"Join the orders table with the customers table to get customer details\",  # Join\n",
    "    (29, \"SELECT\", \"WHERE\"): \"List all records in the products table where price is greater than $20\",  # Where\n",
    "    (30, \"SELECT\", \"ORDER BY\", \"LIMIT\"): \"Show me the names of employees earning the top 5 highest salaries\",  # Order by limit\n",
    "    (31, \"GROUP BY\", \"AGGREGATE\"): \"Find the sum of sales for each region\",  # Group by Sum()\n",
    "    (32, \"SELECT\", \"ORDER BY\"): \"List all products sorted by their price in ascending order\",  # Order by\n",
    "    (33, \"SELECT\", \"EXAMPLE\", \"MONGODB\"): \"Give me examples of nosql queries.\",  # Order by\n",
    "}\n",
    "\n",
    "\n",
    "example_queries[(29, \"SELECT\", \"WHERE\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['show', 'me', 'the', 'total', 'sales', 'grouped', 'by', 'region', 'with', 'a', 'condition', 'on', 'sales']\n",
      "['show', 'total', 'sales', 'grouped', 'region', 'condition', 'sales']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AGGREGATE', 'GROUP BY', 'HAVING', 'WHERE']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def match_query_pattern0(user_input):\n",
    "\n",
    "    tokens = word_tokenize(user_input.lower())\n",
    "    tokens = [token for token in tokens if token.isalnum()]  # Remove punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "                \n",
    "    types = []\n",
    "    for query_type, keywords in QUERY_PATTERNS.items():\n",
    "        for word in tokens:\n",
    "            if word in keywords:\n",
    "                if query_type not in types:\n",
    "                    types.append(query_type)\n",
    "                    pass\n",
    "\n",
    "    if ('HAVING' in types) & ('GROUP BY' not in types):\n",
    "        types.remove('HAVING')\n",
    "    \n",
    "    if ('AGGREGATE' in types) & ('SELECT' in types):\n",
    "        types.remove('SELECT')\n",
    "\n",
    "    print(tokens)\n",
    "    print(filtered_tokens)\n",
    "\n",
    "    return types\n",
    "\n",
    "query = \"Show me the total sales grouped by region with a condition on sales.\"\n",
    "\n",
    "match_query_pattern0(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/jacob/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.11/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.11/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/Users/jacob/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.11/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.11/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m types\n\u001b[1;32m     40\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShow me the total sales grouped by region with a condition on sales.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mmatch_query_pattern1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m, in \u001b[0;36mmatch_query_pattern1\u001b[0;34m(user_input)\u001b[0m\n\u001b[1;32m     14\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;66;03m#.union({\"all\", \"table\", \"in\", \"from\", \"of\", \"for\"})\u001b[39;00m\n\u001b[1;32m     15\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m---> 16\u001b[0m filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Matching with weighted scoring\u001b[39;00m\n\u001b[1;32m     19\u001b[0m type_scores \u001b[38;5;241m=\u001b[39m Counter()\n",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;66;03m#.union({\"all\", \"table\", \"in\", \"from\", \"of\", \"for\"})\u001b[39;00m\n\u001b[1;32m     15\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m---> 16\u001b[0m filtered_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Matching with weighted scoring\u001b[39;00m\n\u001b[1;32m     19\u001b[0m type_scores \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/stem/wordnet.py:85\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` by picking the shortest of the possible lemmas,\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    using the wordnet corpus reader's built-in _morphy function.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    :return: The shortest lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/stem/wordnet.py:41\u001b[0m, in \u001b[0;36mWordNetLemmatizer._morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m_morphy() is WordNet's _morphy lemmatizer.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mIt returns a list of all lemmas found in WordNet.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m['us', 'u']\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet \u001b[38;5;28;01mas\u001b[39;00m wn\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(form, pos, check_exceptions)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/corpus/util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/jacob/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.11/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.11/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, MWETokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "def match_query_pattern1(user_input):\n",
    "    # Tokenization with multi-word expressions\n",
    "    mwe_tokenizer = MWETokenizer([('group', 'by'), ('order', 'by'), \n",
    "                                  ('foreign', 'key'), ('in', 'the'), (\"for\", \"each\")], separator=' ')\n",
    "    tokens = mwe_tokenizer.tokenize(word_tokenize(user_input.lower()))\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    \n",
    "    # Stopwords and lemmatization\n",
    "    stop_words = set(stopwords.words('english'))#.union({\"all\", \"table\", \"in\", \"from\", \"of\", \"for\"})\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Matching with weighted scoring\n",
    "    type_scores = Counter()\n",
    "    for token in filtered_tokens:\n",
    "        for query_type, keywords in QUERY_PATTERNS.items():\n",
    "            if token in keywords:\n",
    "                type_scores[query_type] += 1  # Adjust scoring logic as needed\n",
    "    \n",
    "    # Extract types with the highest scores\n",
    "    types = [query for query, score in type_scores.most_common()]\n",
    "    \n",
    "    # Contextual dependency adjustments\n",
    "    if 'HAVING' in types and 'GROUP BY' not in types:\n",
    "        types.remove('HAVING')\n",
    "    \n",
    "    if 'AGGREGATE' in types and 'SELECT' in types:\n",
    "        types.remove('SELECT')\n",
    "    \n",
    "    print(tokens)\n",
    "    print(filtered_tokens)\n",
    "\n",
    "    return types\n",
    "\n",
    "query = \"Show me the total sales grouped by region with a condition on sales.\"\n",
    "\n",
    "match_query_pattern1(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['show', 'me', 'the', 'total', 'sales', 'grouped', 'by', 'region', 'with', 'a', 'condition', 'on', 'sales']\n",
      "['show', 'total', 'sales', 'grouped', 'region', 'condition', 'sales']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['WHERE', 'HAVING', 'AGGREGATE', 'GROUP BY']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def match_query_pattern2(user_input):\n",
    "    from nltk.util import ngrams\n",
    "    from nltk.corpus import wordnet as wn\n",
    "\n",
    "    tokens = word_tokenize(user_input.lower())\n",
    "    tokens = [token for token in tokens if token.isalnum()]  # Remove punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Detect n-grams (e.g., \"group by\")\n",
    "    bigrams = list(ngrams(tokens, 2))\n",
    "    phrases = [\" \".join(bigram) for bigram in bigrams]\n",
    "\n",
    "    # Match tokens and phrases to query patterns\n",
    "    detected_types = []\n",
    "    for query_type, keywords in QUERY_PATTERNS.items():\n",
    "        for word in filtered_tokens + phrases:\n",
    "            if word in keywords:\n",
    "                detected_types.append(query_type)\n",
    "                break\n",
    "\n",
    "    # Handle contextual relationships\n",
    "    if \"HAVING\" in detected_types and \"GROUP BY\" not in detected_types:\n",
    "        detected_types.remove(\"HAVING\")\n",
    "    \n",
    "    if \"AGGREGATE\" in detected_types and \"SELECT\" in detected_types:\n",
    "        detected_types.remove(\"SELECT\")\n",
    "    \n",
    "    print(tokens)\n",
    "    print(filtered_tokens)\n",
    "\n",
    "    return list(set(detected_types))\n",
    "\n",
    "query = \"Show me the total sales grouped by region with a condition on sales.\"\n",
    "\n",
    "match_query_pattern2(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUP BY\n",
      "['find', 'average', 'salary', 'employees', 'department']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- 0\n",
      "['show', 'me', 'all', 'the', 'records', 'in', 'the', 'employees', 'table']\n",
      "['show', 'records', 'employees', 'table']\n",
      "Guess: ['SELECT']\n",
      "Ans: ['SELECT']\n",
      "--------- 1\n",
      "['upload', 'this', 'data']\n",
      "['upload', 'data']\n",
      "Guess: ['UPLOAD']\n",
      "Ans: ['UPLOAD']\n",
      "--------- 2\n",
      "['show', 'me', 'examples', 'of', 'sql', 'queries']\n",
      "['show', 'examples', 'sql', 'queries']\n",
      "Guess: ['EXAMPLE', 'SELECT', 'SQL']\n",
      "Ans: ['EXAMPLE', 'SELECT', 'SQL']\n",
      "--------- 3\n",
      "['show', 'me', 'examples', 'of', 'sql', 'queries', 'using', 'group', 'by']\n",
      "['show', 'examples', 'sql', 'queries', 'using', 'group']\n",
      "Guess: ['EXAMPLE', 'GROUP BY', 'SELECT', 'SQL']\n",
      "Ans: ['EXAMPLE', 'GROUP BY', 'SELECT', 'SQL']\n",
      "--------- 4\n",
      "['get', 'the', 'details', 'of', 'employees', 'who', 'work', 'in', 'the', 'sales', 'department']\n",
      "['get', 'details', 'employees', 'work', 'sales', 'department']\n",
      "Guess: ['SELECT', 'WHERE']\n",
      "Ans: ['SELECT', 'WHERE']\n",
      "--------- 5\n",
      "['how', 'many', 'employees', 'are', 'there', 'in', 'the', 'company']\n",
      "['many', 'employees', 'company']\n",
      "Guess: ['AGGREGATE']\n",
      "Ans: ['COUNT', 'SELECT']\n",
      "--------- 6\n",
      "['show', 'me', 'the', 'names', 'and', 'salaries', 'of', 'employees', 'earning', 'more', 'than', '50000']\n",
      "['show', 'names', 'salaries', 'employees', 'earning', '50000']\n",
      "Guess: ['SELECT']\n",
      "Ans: ['SELECT', 'WHERE']\n",
      "--------- 7\n",
      "['list', 'all', 'employees', 'sorted', 'by', 'their', 'hire', 'date', 'in', 'descending', 'order']\n",
      "['list', 'employees', 'sorted', 'hire', 'date', 'descending', 'order']\n",
      "Guess: ['ORDER BY', 'SELECT']\n",
      "Ans: ['ORDER BY', 'SELECT']\n",
      "--------- 8\n",
      "['find', 'the', 'average', 'salary', 'of', 'employees', 'in', 'the', 'engineering', 'department']\n",
      "['find', 'average', 'salary', 'employees', 'engineering', 'department']\n",
      "Guess: ['AGGREGATE']\n",
      "Ans: ['AGGREGATE', 'WHERE']\n",
      "--------- 9\n",
      "['how', 'many', 'employees', 'are', 'there', 'in', 'each', 'department']\n",
      "['many', 'employees', 'department']\n",
      "Guess: ['AGGREGATE', 'GROUP BY']\n",
      "Ans: ['GROUP BY', 'SELECT']\n",
      "--------- 10\n",
      "['show', 'me', 'the', 'departments', 'where', 'the', 'total', 'salaries', 'of', 'employees', 'exceed', '100000']\n",
      "['show', 'departments', 'total', 'salaries', 'employees', 'exceed', '100000']\n",
      "Guess: ['AGGREGATE', 'GROUP BY', 'WHERE']\n",
      "Ans: ['HAVING', 'SELECT']\n",
      "--------- 11\n",
      "['list', 'the', 'products', 'where', 'the', 'average', 'price', 'is', 'less', 'than', '50']\n",
      "['list', 'products', 'average', 'price', 'less', '50']\n",
      "Guess: ['AGGREGATE', 'WHERE']\n",
      "Ans: ['HAVING', 'SELECT']\n",
      "--------- 12\n",
      "['what', 'are', 'the', 'top', '10', 'highest', 'paid', 'employees']\n",
      "['top', '10', 'highest', 'paid', 'employees']\n",
      "Guess: ['LIMIT', 'ORDER BY']\n",
      "Ans: ['LIMIT', 'ORDER BY', 'SELECT']\n",
      "--------- 13\n",
      "['which', 'customers', 'have', 'placed', 'more', 'than', '5', 'orders']\n",
      "['customers', 'placed', '5', 'orders']\n",
      "Guess: ['WHERE']\n",
      "Ans: ['HAVING', 'SELECT']\n",
      "--------- 14\n",
      "['find', 'the', 'total', 'revenue', 'generated', 'by', 'each', 'product', 'category']\n",
      "['find', 'total', 'revenue', 'generated', 'product', 'category']\n",
      "Guess: ['AGGREGATE', 'GROUP BY']\n",
      "Ans: ['GROUP BY', 'SELECT']\n",
      "--------- 15\n",
      "['list', 'all', 'orders', 'placed', 'in', 'the', 'last', '30', 'days', 'sorted', 'by', 'order', 'date']\n",
      "['list', 'orders', 'placed', 'last', '30', 'days', 'sorted', 'order', 'date']\n",
      "Guess: ['ORDER BY', 'SELECT']\n",
      "Ans: ['ORDER BY', 'SELECT', 'WHERE']\n",
      "--------- 16\n",
      "['join', 'the', 'employees', 'table', 'with', 'the', 'departments', 'table', 'to', 'find', 'department', 'names']\n",
      "['join', 'employees', 'table', 'departments', 'table', 'find', 'department', 'names']\n",
      "Guess: ['JOIN']\n",
      "Ans: ['JOIN', 'SELECT']\n",
      "--------- 17\n",
      "['list', 'all', 'customers', 'and', 'their', 'orders']\n",
      "['list', 'customers', 'orders']\n",
      "Guess: ['SELECT']\n",
      "Ans: ['JOIN', 'SELECT']\n",
      "--------- 18\n",
      "['how', 'many', 'products', 'are', 'in', 'stock', 'for', 'each', 'supplier']\n",
      "['many', 'products', 'stock', 'supplier']\n",
      "Guess: ['AGGREGATE', 'GROUP BY']\n",
      "Ans: ['GROUP BY', 'SELECT']\n",
      "--------- 19\n",
      "['show', 'me', 'the', 'products', 'where', 'the', 'stock', 'quantity', 'is', 'less', 'than', '10']\n",
      "['show', 'products', 'stock', 'quantity', 'less', '10']\n",
      "Guess: ['SELECT', 'WHERE']\n",
      "Ans: ['SELECT', 'WHERE']\n",
      "--------- 20\n",
      "['upload', 'the', 'file', 'into', 'the', 'database']\n",
      "['upload', 'file', 'database']\n",
      "Guess: ['UPLOAD']\n",
      "Ans: ['UPLOAD']\n",
      "--------- 21\n",
      "['find', 'the', 'maximum', 'salary', 'in', 'each', 'department']\n",
      "['find', 'maximum', 'salary', 'department']\n",
      "Guess: ['AGGREGATE', 'GROUP BY']\n",
      "Ans: ['AGGREGATE', 'GROUP BY']\n",
      "--------- 22\n",
      "['list', 'employees', 'grouped', 'by', 'their', 'job', 'titles']\n",
      "['list', 'employees', 'grouped', 'job', 'titles']\n",
      "Guess: ['SELECT']\n",
      "Ans: ['GROUP BY', 'SELECT']\n",
      "--------- 23\n",
      "['show', 'me', 'all', 'orders', 'where', 'the', 'total', 'exceeds', '1000']\n",
      "['show', 'orders', 'total', 'exceeds', '1000']\n",
      "Guess: ['AGGREGATE', 'GROUP BY', 'WHERE']\n",
      "Ans: ['SELECT', 'WHERE']\n",
      "--------- 24\n",
      "['list', 'all', 'customers', 'sorted', 'by', 'their', 'last', 'purchase', 'date']\n",
      "['list', 'customers', 'sorted', 'last', 'purchase', 'date']\n",
      "Guess: ['ORDER BY', 'SELECT']\n",
      "Ans: ['ORDER BY', 'SELECT']\n",
      "--------- 25\n",
      "['show', 'me', 'examples', 'of', 'sql', 'queries', 'for', 'finding', 'duplicates']\n",
      "['show', 'examples', 'sql', 'queries', 'finding', 'duplicates']\n",
      "Guess: ['EXAMPLE', 'SELECT', 'SQL']\n",
      "Ans: ['EXAMPLE', 'SELECT', 'SQL']\n",
      "--------- 26\n",
      "['how', 'many', 'employees', 'were', 'hired', 'in', 'the', 'last', 'year']\n",
      "['many', 'employees', 'hired', 'last', 'year']\n",
      "Guess: ['AGGREGATE']\n",
      "Ans: ['AGGREGATE', 'WHERE']\n",
      "--------- 27\n",
      "['find', 'the', 'minimum', 'price', 'of', 'products', 'in', 'each', 'category']\n",
      "['find', 'minimum', 'price', 'products', 'category']\n",
      "Guess: ['AGGREGATE', 'GROUP BY']\n",
      "Ans: ['AGGREGATE', 'GROUP BY']\n",
      "--------- 28\n",
      "['join', 'the', 'orders', 'table', 'with', 'the', 'customers', 'table', 'to', 'get', 'customer', 'details']\n",
      "['join', 'orders', 'table', 'customers', 'table', 'get', 'customer', 'details']\n",
      "Guess: ['JOIN', 'SELECT']\n",
      "Ans: ['JOIN', 'SELECT']\n",
      "--------- 29\n",
      "['list', 'all', 'records', 'in', 'the', 'products', 'table', 'where', 'price', 'is', 'greater', 'than', '20']\n",
      "['list', 'records', 'products', 'table', 'price', 'greater', '20']\n",
      "Guess: ['SELECT', 'WHERE']\n",
      "Ans: ['SELECT', 'WHERE']\n",
      "--------- 30\n",
      "['show', 'me', 'the', 'names', 'of', 'employees', 'earning', 'the', 'top', '5', 'highest', 'salaries']\n",
      "['show', 'names', 'employees', 'earning', 'top', '5', 'highest', 'salaries']\n",
      "Guess: ['LIMIT', 'ORDER BY', 'SELECT']\n",
      "Ans: ['LIMIT', 'ORDER BY', 'SELECT']\n",
      "--------- 31\n",
      "['find', 'the', 'sum', 'of', 'sales', 'for', 'each', 'region']\n",
      "['find', 'sum', 'sales', 'region']\n",
      "Guess: ['AGGREGATE', 'GROUP BY']\n",
      "Ans: ['AGGREGATE', 'GROUP BY']\n",
      "--------- 32\n",
      "['list', 'all', 'products', 'sorted', 'by', 'their', 'price', 'in', 'ascending', 'order']\n",
      "['list', 'products', 'sorted', 'price', 'ascending', 'order']\n",
      "Guess: ['ORDER BY', 'SELECT']\n",
      "Ans: ['ORDER BY', 'SELECT']\n",
      "--------- 33\n",
      "['give', 'me', 'examples', 'of', 'nosql', 'queries']\n",
      "['give', 'examples', 'nosql', 'queries']\n",
      "Guess: ['EXAMPLE', 'MONGODB']\n",
      "Ans: ['EXAMPLE', 'MONGODB', 'SELECT']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.5)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits = []\n",
    "\n",
    "for j, (t, query) in enumerate(example_queries.items()):\n",
    "    tokens = word_tokenize(query.lower())\n",
    "    tokens = [token for token in tokens if token.isalnum()]  # Remove punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    print(\"---------\", j)\n",
    "    guess = sorted(match_query_pattern0(query))\n",
    "    # intersection = sorted(list(set(match_query_pattern0(query)).intersection(match_query_pattern1(query), match_query_pattern2(query))))\n",
    "    # print('Inter:', intersection)\n",
    "    print('Guess:', guess)\n",
    "    t = sorted(list(t[1:]))\n",
    "    print('Ans:', t)\n",
    "    hits.append(guess == t)\n",
    "\n",
    "np.mean(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SELECT', 'ORDER BY')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = ['SELECT']\n",
    "# b = ('SELECT',)\n",
    "\n",
    "# a == list(b)\n",
    "t[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
